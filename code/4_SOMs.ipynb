{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Organized Maps (SOMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# SimpSOM, https://simpsom.readthedocs.io\n",
    "import simpsom as sps\n",
    "# from simpsom.plots import scatter_on_map\n",
    "\n",
    "# Astropy\n",
    "from astropy.table import Table\n",
    "\n",
    "# CuPY\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RADIUS (3''), EMU (1''), DESY6GOLD (0.1''), VIKINGDR5 (0.1''), CATWISE2020 (0.2''), MAGNITUDES\n",
    "# For a false detection rate of <3% \n",
    "# p_i>=0.1 & match_flag==1 & p_any>0.85\n",
    "emu_matched_catalog_path_1 = '/data/mfonseca/cross_match/EMU_0102-32_DESY6GOLD_VIKING_CATWISE_Mags/EMU_0102-32_DESY6GOLD_VIKING_CATWISE_Mags.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emu_matched_catalog_path = '/data/mfonseca/cross_match/EMU_0102-32_DESY6GOLD_VIKING_CATWISE_noMags/EMU_0102-32_DESY6GOLD_VIKINGDR5_CATWISE_noMag_fakematch.fits'\n",
    "emu_matched_catalog = Table.read(emu_matched_catalog_path)\n",
    "emu_matched_catalog = emu_matched_catalog.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emu_matched_catalog.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we filter the NWAY catalog to keep only the best objects\n",
    "\n",
    "# From NWAY\n",
    "# Filter the catalog based on the match_flag and p_i, p_any values\n",
    "match_mask = (emu_matched_catalog['match_flag'] == 1) & (emu_matched_catalog['p_i'] >= 0.1) & (emu_matched_catalog['p_any'] >=0.81)\n",
    "catalog_matched = emu_matched_catalog[match_mask]\n",
    "print(f'Number of objects in the catalog {len(catalog_matched)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we select objects that have matches in all three surveys\n",
    "# full_match_mask = (catalog_matched['DES_coadd_object_id'] != -99) & (catalog_matched['VKG_sourceID'] != -99) & (catalog_matched['CAT_source_id'] != -99)\n",
    "full_match_mask = (catalog_matched['DESY6_coadd_object_id'] != -99) & (catalog_matched['VKG_sourceID'] != -99) & (catalog_matched['CAT_source_id'] != -99)\n",
    "catalog_matched = catalog_matched[full_match_mask]\n",
    "print(f'Number of objects in the catalog after filtering {len(catalog_matched)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the multiwavelenght catalog to keep only the columns needed\n",
    "emu_columns = ['EMU_island_id']\n",
    "\n",
    "# desdr2_mag = ['DES_mag_auto_g_dered', 'DES_mag_auto_r_dered', 'DES_mag_auto_i_dered', 'DES_mag_auto_z_dered']\n",
    "# desdr2_colors = ['DES_g_r_dered', 'DES_r_i_dered', 'DES_i_z_dered']\n",
    "\n",
    "desy6gold_mag = ['DESY6_mag_auto_g_extcorr', 'DESY6_mag_auto_r_extcorr', 'DESY6_mag_auto_i_extcorr', 'DESY6_mag_auto_z_extcorr', 'DESY6_mag_auto_y_extcorr']\n",
    "desy6gold_colors = ['DESY6_g_r_extcorr', 'DESY6_r_i_extcorr', 'DESY6_i_z_extcorr', 'DESY6_z_y_extcorr']\n",
    "\n",
    "viking_mag = ['VKG_zAperMag3_ab_extcorr', 'VKG_jAperMag3_ab_extcorr', 'VKG_yAperMag3_ab_extcorr', 'VKG_ksAperMag3_ab_extcorr', 'VKG_hAperMag3_ab_extcorr']\n",
    "viking_colors = ['VKG_z_y_am3_extcorr', 'VKG_y_j_am3_extcorr', 'VKG_j_h_am3_extcorr', 'VKG_h_ks_am3_extcorr']\n",
    "\n",
    "catwise_mag = ['CAT_W1mpro_ab', 'CAT_W2mpro_ab']\n",
    "catwise_colors = ['CAT_w1_w2_ab']\n",
    "\n",
    "magnitude_columns = emu_columns + desy6gold_colors + viking_colors\n",
    "\n",
    "catalog_matched = catalog_matched[magnitude_columns]\n",
    "\n",
    "# Should be the same lenght as the filtered catalog\n",
    "print(len(catalog_matched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of each columns\n",
    "\n",
    "# for col in catalog_filtered_magnitude.columns:\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.hist(catalog_filtered_magnitude[col], bins=100, color='blue', alpha=0.7)\n",
    "#     plt.title(f'Histogram of {col}')\n",
    "#     plt.xlabel(col)\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.grid()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the topology of the SOM, follow indications from \"Self-Organizing Maps and Their Applications to Data Analysis\" R. Ponmalai, C. Kamath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOM topology (lattice size)\n",
    "\n",
    "# Number of sources in our dataframe\n",
    "inputdata_size = len(catalog_matched)\n",
    "\n",
    "# From Kohonen, the optimal ratio of height to width of the lattice\n",
    "# is equal to the ratio of the two largest eigenvalue of the autocorrelation matrix\n",
    "cov_std = catalog_matched.cov()\n",
    "eigen_values = np.linalg.eigvals(cov_std.values)\n",
    "eigen_values_sorted = np.sort(eigen_values)[::-1]\n",
    "ratio_eigenvalues = eigen_values_sorted[0] / eigen_values_sorted[1]\n",
    "\n",
    "# From Kohonen, the number of nodes if 5*sqrt(n), where n is the number of data points\n",
    "som_dim = int(np.sqrt(inputdata_size) * 5)\n",
    "\n",
    "# The number of nodes in the x and y direction\n",
    "som_x = int(np.sqrt(som_dim * ratio_eigenvalues))\n",
    "som_y = int(som_x / ratio_eigenvalues)\n",
    "\n",
    "#=================================================================\n",
    "\n",
    "# Print the results\n",
    "print('Dimension of the SOM: ', som_dim)\n",
    "print('Square SOM map size ', np.round(np.sqrt(som_dim)))\n",
    "print('SOM map size using eigenvalues: ', som_x, 'x', som_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the SOM, if wanted, keeping the eigenvalue ratio\n",
    "\n",
    "# som_x = \n",
    "# som_y = int(som_x / ratio_eigenvalues)\n",
    "# som_dim = som_x * som_y\n",
    "\n",
    "#=================================================================\n",
    "\n",
    "# Print the results\n",
    "print('Dimension of the SOM: ', som_dim)\n",
    "print('Square SOM map size ', np.round(np.sqrt(som_dim)))\n",
    "print('SOM map size using eigenvalues: ', som_x, 'x', som_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definine the subset of the catalog to use\n",
    "data_fraction2use = 1\n",
    "data = catalog_matched.sample(frac=data_fraction2use)\n",
    "\n",
    "# Normalize the data\n",
    "data.to_numpy()\n",
    "data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "data = data.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Error (QE) and Topographic Error (TE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION WORKS\n",
    "\n",
    "# Quantization Error (QE)\n",
    "# Average distance of a data point to the nearest lattice node\n",
    "# Measures how well the mapping fits the distribution of the data\n",
    "\n",
    "def som_quantization_error(data, trained_som):\n",
    "    '''\n",
    "    Calculates the quantization error of a trained SOM.ipynb\n",
    "\n",
    "    Args:\n",
    "        data: NumPy array of input data.\n",
    "        trainded_som: trained SimpSOM SOMNet object\n",
    "\n",
    "    Returns:\n",
    "        quantization error (float)\n",
    "    '''\n",
    "    # Convert to clean NumPy array with uniform dtype first\n",
    "    data_np = np.asarray(data, dtype=np.float64)\n",
    "\n",
    "    # Convert NumPy array to CuPy array for GPU processing\n",
    "    data_cp = cp.array(data_np)\n",
    "\n",
    "    # Find all BMU indices\n",
    "    bmu_indices = trained_som.find_bmu_ix(data_cp)\n",
    "\n",
    "    # Get the weights of the BMUs\n",
    "    bmu_weights = cp.array([trained_som.nodes_list[int(bmu_idx)].weights for bmu_idx in bmu_indices])\n",
    "\n",
    "    # Vectorized distance calculation\n",
    "    distances = cp.linalg.norm(bmu_weights - data_cp, axis=1)\n",
    "    total_distance = cp.sum(distances)\n",
    "\n",
    "    quantization_error = float(total_distance / len(data))\n",
    "    return quantization_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION WORKS\n",
    "\n",
    "# Topographic Error (TE)\n",
    "# Proportion of data points whose BMU and second BMU are NOT neighbors\n",
    "# Measures how well the shape of the data is preserved in the output space\n",
    "\n",
    "def som_topographic_error(data, trained_som):\n",
    "    \"\"\"\n",
    "    Computes topographic error ET for a SOM with hex topology using GPU (CuPy).\n",
    "    \n",
    "    Args:\n",
    "        data:    The input data used to train the SOM (NumPy array).\n",
    "        trained_som: A trained SOMNet object from the simpsom library.\n",
    "\n",
    "    Returns:\n",
    "        Topographic error (float between 0 and 1).\n",
    "        Proportion of data points whose BMU and second BMU are NOT neighbors\n",
    "    \"\"\"\n",
    "    # Convert to clean NumPy array with uniform dtype first\n",
    "    data_np = np.asarray(data, dtype=np.float64)\n",
    "\n",
    "    # Convert data to CuPy arrays\n",
    "    data_cp = cp.array(data)\n",
    "\n",
    "    # Get all the nodes' weights from the trained SOM (GPU-enabled)\n",
    "    weights_cp = cp.array([node.weights for node in trained_som.nodes_list])\n",
    "\n",
    "    # Initialize total error\n",
    "    total_error = 0\n",
    "\n",
    "    # Find all BMU indices\n",
    "    bmu_indices = trained_som.find_bmu_ix(data_cp)\n",
    "\n",
    "    # Find all the second BMU indices\n",
    "    som_dist = sps.distances.Distance(xp=np) # Initialize the distance object, as required by the documentation\n",
    "    distances = som_dist.pairdist(data_cp, weights_cp, metric='euclidean')\n",
    "\n",
    "    sorted_indices = cp.argsort(distances, axis=1)\n",
    "    sbmu_indices = sorted_indices[:, 1]\n",
    "\n",
    "\n",
    "    # Get the positions of the BMU and second BMU in the grid\n",
    "    bmu_positions = cp.array([trained_som.nodes_list[int(bmu_idx)].pos for bmu_idx in bmu_indices])\n",
    "    sbmu_positions = cp.array([trained_som.nodes_list[int(sbmu_idx)].pos for sbmu_idx in sbmu_indices])\n",
    "\n",
    "\n",
    "    bmu_row, bmu_col = bmu_positions[:, 0], bmu_positions[:, 1]\n",
    "    sbmu_row, sbmu_col = sbmu_positions[:, 0], sbmu_positions[:, 1]\n",
    "\n",
    "\n",
    "    # Check if BMU and sBMU are neighbors\n",
    "    row_neighbors = cp.abs(bmu_row - sbmu_row)\n",
    "    col_neighbors = cp.abs(bmu_col - sbmu_col)\n",
    "\n",
    "    not_neighbors = (row_neighbors > 1) | (col_neighbors > 1)\n",
    "\n",
    "    number_not_neighbors = cp.sum(not_neighbors)\n",
    "\n",
    "    # Compute the topographic error\n",
    "    topographic_error = number_not_neighbors / data_cp.shape[0]\n",
    "\n",
    "    return float(topographic_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work correcly because the learning rate is not updated in each training epoch\n",
    "\n",
    "def track_som_errors(data, epochs, net_height=20, net_width=20):\n",
    "    som_net = sps.SOMNet(\n",
    "        net_height=net_height,\n",
    "        net_width=net_width,\n",
    "        data=data,\n",
    "        topology='hexagonal',\n",
    "        init='PCA',\n",
    "        metric='euclidean',\n",
    "        neighborhood_fun='gaussian',\n",
    "        PBC=True,\n",
    "        GPU=True,\n",
    "        random_seed=42,\n",
    "        output_path=\"./\"\n",
    "    )\n",
    "\n",
    "    quantization_errors = []\n",
    "    topographic_errors = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        som_net.train(\n",
    "            train_algo='batch',\n",
    "            epochs=1,\n",
    "            start_learning_rate = 0.1,\n",
    "            early_stop=None,\n",
    "            batch_size=-1\n",
    "        )\n",
    "\n",
    "        quant_error = som_quantization_error(data, som_net)\n",
    "        topo_error = som_topographic_error(data, som_net)\n",
    "\n",
    "        quantization_errors.append(quant_error)\n",
    "        topographic_errors.append(topo_error)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | QE: {quant_error:.4f} | TE: {topo_error:.4f}\")\n",
    "\n",
    "    return quantization_errors, topographic_errors, som_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SOM\n",
    "som = sps.SOMNet(\n",
    "    som_x,\n",
    "    som_y,\n",
    "    data,\n",
    "    load_file = None,\n",
    "    metric= 'euclidean', # 'euclidian' or 'cosine' or 'manhattan'\n",
    "    # metrick_kwds={},\n",
    "    init = 'random' , # 'random' or 'pca'\n",
    "    PBC = True,\n",
    "    GPU = True,\n",
    "    random_seed = 69,\n",
    "    topology='hexagonal', # 'square' or 'hexagonal'\n",
    "    debug= True\n",
    ")\n",
    "\n",
    "som.train(\n",
    "    train_algo = 'batch', \n",
    "    epochs = 200,\n",
    "    # start_learning_rate = 0.01 , # Used only for online training\n",
    "    early_stop = None ,\n",
    "    early_stop_patience = 5 ,\n",
    "    early_stop_tolerance = 0.01 ,\n",
    "    batch_size = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.save_map('/data/mfonseca/soms/test1.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.plot_map_by_difference(show=True, print_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Quantization Error and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(net.quantization_error, label='Quantization Error', color='blue')\n",
    "plt.plot(net.topographic_error, label='Topographic Error', color='orange')\n",
    "plt.title('SOM Training Errors')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(catalog_filtered_magnitude.columns)):\n",
    "    som.plot_map_by_feature(\n",
    "        feature_ix= i,\n",
    "        show=True,\n",
    "        print_out=True,\n",
    "        cbar_label=catalog_filtered_magnitude.columns[i],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniSom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt', \n",
    "                    names=['area', 'perimeter', 'compactness', 'length_kernel', 'width_kernel',\n",
    "                   'asymmetry_coefficient', 'length_kernel_groove', 'target'], usecols=[0, 5], \n",
    "                   sep='\\t+', engine='python')\n",
    "# data normalization\n",
    "data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "data = data.values\n",
    "\n",
    "\n",
    "# Initialization and training\n",
    "som_shape = (1, 3)\n",
    "som = MiniSom(som_shape[0], som_shape[1], data.shape[1], sigma=.5, learning_rate=.5,\n",
    "              neighborhood_function='gaussian', random_seed=10)\n",
    "\n",
    "som.train_batch(data, 500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each neuron represents a cluster\n",
    "winner_coordinates = np.array([som.winner(x) for x in data]).T\n",
    "# with np.ravel_multi_index we convert the bidimensional\n",
    "# coordinates to a monodimensional index\n",
    "cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the clusters using the first 2 dimentions of the data\n",
    "for c in np.unique(cluster_index):\n",
    "    plt.scatter(data[cluster_index == c, 0],\n",
    "                data[cluster_index == c, 1], label='cluster='+str(c), alpha=.7)\n",
    "\n",
    "# plotting centroids\n",
    "for centroid in som.get_weights():\n",
    "    plt.scatter(centroid[:, 0], centroid[:, 1], marker='x', \n",
    "                s=4, linewidths=10, color='k', label='centroid')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With our galaxy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_names = catalog_df['EMU_island_name'].str.decode('utf-8').str.strip()\n",
    "\n",
    "# Normalize the data\n",
    "data = (catalog_df_filtered_matches_magnitude - np.mean(catalog_df_filtered_matches_magnitude, axis=0)) / np.std(catalog_df_filtered_matches_magnitude, axis=0)\n",
    "data = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som_shape = (10, 10)\n",
    "som = MiniSom(som_shape[0], som_shape[1], data.shape[1], sigma=.5, learning_rate=.1,\n",
    "              neighborhood_function='gaussian', random_seed=0)\n",
    "\n",
    "som.train_batch(data, 100000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "frequencies = som.activation_response(data)\n",
    "plt.pcolor(frequencies.T, cmap='Blues') \n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each neuron represents a cluster\n",
    "winner_coordinates = np.array([som.winner(x) for x in data]).T\n",
    "# with np.ravel_multi_index we convert the bidimensional\n",
    "# coordinates to a monodimensional index\n",
    "cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the clusters using the first 2 dimentions of the data\n",
    "for c in np.unique(cluster_index):\n",
    "    plt.scatter(data[cluster_index == c, 0],\n",
    "                data[cluster_index == c, 1], label='cluster='+str(c), alpha=.7)\n",
    "\n",
    "# plt.xlim(-.5,1)\n",
    "# plt.ylim(-1,1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
